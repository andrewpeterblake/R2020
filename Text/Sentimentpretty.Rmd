---
title: "Text modelling of sentiment"
author: "Andrew P Blake"
date: "October 2020"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
bibliography: Textmining.bib
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
```

> _Disclaimer: The Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the individual presenter and not the wider Bank of England or its Policy Committees._

-------

## Text as data

- The algorithmic analysis of textual data is relatively new to economics, but has already had some major successes.
- Particularly true for topics of interest to central banks:
    - @Bloom created an uncertainty index from newspaper archives, with one measure including mentions of 'Federal Reserve'.
    - @HMP exploit a fortunate natural experiment involving the FOMC minutes to investigate the impact of transparency on policy making.
    - @JSS look at how the textual analysis of the _Inflation Report_ enhances Bank of England growth forecasts.
    - @FSRnews look to see how much sentiment in the Financial Stability Report leads or is driven by the market.
    - @Nyman use it to examine if periods of high in excitement and low anxiety are an important warning sign of impending financial system distress.
    - Not exhaustive -- @Canada, @Correa, lots of Bank of England work...
- @NBERw23276 discuss a variety of applications that have traction in economics, including forecasting. The excellent @Taddy has some good material on text amongst other things.

### Being Tidy

Excellent 'cookbook' by [Julia Silge](https://juliasilge.com/) and [David Robinson](http://varianceexplained.org/) @Silge, available for free [online](http://tidytextmining.com/). They illustrate methods using many examples including the complete works of Jane Austen. Both of them do fantastic online tutorials, and I can't recommend them highly enough. Julia Silge is heavily involved in the `tidymodels` project and on her [blog](https://juliasilge.com/blog/) has put a really great sequence of videos on many aspects of that and text modelling.

## Text as data

- Basic data is words; we need to put those words into Tidy format.
- Generally creating the right form that we want to analyze is called _tokenization_.
- Data is often in document form and in particular in Adobe PDF format.
- Need to make use of tools to read such documents and turn them into text.
- Not so easy but good library available in `pdftools`:
     - Use this library to read the text but then we need to 'clean up'.
     - Precise details of this will likely depend on the system used.
- Other formats need different tools -- we often need to do a little bit of web scraping too.
- And, repeat, any resulting text always needs **cleaning**.

## Counting words

```{r libs, message=FALSE, warning=FALSE}
library(pdftools)
library(stringi)

library(tidyverse)
library(tidytext)
library(SnowballC)
library(lubridate)
library(readxl)
library(readr)
library(patchwork)

library(knitr)
library(kableExtra)
```

### A financial stability dictionary

We download the @Correa dictionary (later 'Fed') for sentiment analysis, discussed below.

```{r frb, message=FALSE}
ifdpfiles <- "https://www.federalreserve.gov/econres/ifdp/files/"
ifdp1203  <- "ifdp1203-appendix.xlsx"
download.file(paste0(ifdpfiles,ifdp1203), mode="wb", destfile=ifdp1203)

# Store dictionary in same format as tidytext
Sdict <- bind_rows(mutate(read_excel(ifdp1203, sheet=2), 
                          Mood=ifelse(is.na(Positive),"Negative","Positive")), 
                   mutate(read_excel(ifdp1203, sheet=4), 
                          Mood="Neutral")) %>% 
  select(word=Word, Mood) 
```

## The Canadian MPR

We take Canada as our main example, because they have nice reports...


```{r}
text_to_frame = function(pdf_file) {
  
  date  <- as.Date(substr(pdf_file,5,14))
  mths  <- month.abb[month(date)]
  yrs   <- year(date)

  MPR <- suppressMessages(pdf_text(pdf_file))
  MPR <- gsub("\r\n"," ",MPR)
  MPR <- gsub("\t"," ",MPR)
  # MPR <- stringi::stri_escape_unicode(MPR)
  MPR <- stri_trans_general(MPR, "latin-ascii")
  MPR <- iconv(MPR, "UTF-8", "ASCII", sub = " ")
  MPR <- gsub("[[:digit:]]+"," ",MPR)
  # MPR <- gsub("[[:punct:]]+"," ",MPR)
  MPR <- gsub("[[:space:]]+"," ",MPR)
  MPR <- tolower(trimws(MPR))
  MPR <- gsub("monetary policy report"," ",MPR)
  MPR <- gsub("onetary policy report"," ",MPR)
  MPR <- gsub("netary policy report"," ",MPR)
  MPR <- gsub("m o n e t a r y p o l i c y r e p o r t","",MPR)
  MPR <- gsub("r e p o r t o n m o n e t a r y p o l i c y","",MPR)
  MPR <- gsub("bank of canada"," ",MPR)
  MPR <- gsub("b a n k o f c a n a d a","",MPR)
  MPR <- gsub("[[:space:]]+"," ",MPR)
  MPR <- gsub("file information for internal use only"," ",MPR)
  
  MPR <- trimws(MPR)
  MPR_add <- tibble(page=1:length(MPR),
                    document=pdf_file,
                    year=year(date),
                    month=mths,
                    text=MPR) 
  
  gg <- MPR_add$text
  lm <- tolower(month.name[month(date)])
  cm <- paste0("^", lm)
  for (l in 1:2) {
    gg <- trimws(gsub("^global economy", "", gg))
    gg <- trimws(gsub("^canadian economy", "", gg))
    gg <- trimws(gsub("^appendix", "", gg))
    gg <- trimws(gsub("^update", "", gg))
    gg <- trimws(gsub("^risks to the inflation outlook", "", gg))
    gg <- trimws(gsub("^reassessment of canadian potential output growth", "", gg))
    gg <- trimws(gsub(cm, "", gg))
    gg <- trimws(gsub(trimws(gsub("", " ", lm)), "", gg))
    gg <- trimws(gsub("^chart", "", gg))
    gg <- trimws(gsub("^box", "", gg))
    gg <- trimws(gsub("^table", "", gg))
  }
  gg <- trimws(gsub("bankofcanada ca", "", gg))
  wr <- which(word(gg) == "bibliography")
  if (length(wr) < 1) wr <- which(word(gg) == "references") 

  MPR_add$text <- gg
  
  if (length(wr) > 0) MPR_add <- MPR_add[1:(wr-1),]
  
  MPR_add <- MPR_add %>% 
    filter(word(text, 1) != "") %>% 
    filter(word(text, 1) != "contents") %>% 
    filter(word(text, 5, 6) != "sixtieth anniversary") %>% 
    filter(word(text, 1, 3) != "the silver dollar") %>% 
    filter(word(text, 2, 3) != "sterling silver") %>% 
    filter(word(text, 2, 3) != "gold coin") %>% 
    filter(word(text, 1, 2) != "gold coin") %>% 
    filter(word(text, 1, 3) != "library of parliament") %>% 
    filter(word(text, 1, 4) != "this is a report") %>% 
    filter(word(text, 1, 5) != "this text is a commentary") %>% 
    filter(word(text, 1, 5) != "canada s inflation control strategy")

  return(MPR_add)
}
```



```{r read}
# Dates for downloaded files

files <- list.files(pattern = "^mpr")
date  <- as.Date(substr(files,5,14))
mths  <- month.abb[month(date)]
yrs   <- year(date)

s_score_fed <- list()
s_score_a   <- list()
s_score_b   <- list()

pts     <- list()
igram   <- list()

for(j in 1:length(files)) {

  MPR <- text_to_frame(files[[j]]) # %>% 
    # mmutate(text = gsub("[[:punct:]]+", " ", text))

  # Unnest tokens to single words
  tidy_MPR <- MPR %>% 
    unnest_tokens(word, text) %>% 
    anti_join(stop_words, by="word") 
  
  # Count 'inflation'
  igram[[j]] <- tidy_MPR %>% 
    summarise(pi = 100*sum(word == "inflation")/n()) %>% 
    mutate(Date = date[j]) 
  
  # Popular word plots
  pts[[j]] <- tidy_MPR %>% 
    mutate(word = wordStem(word)) %>% 
    count(word, sort=TRUE) %>%
    mutate(proportion = n/sum(n)) %>%
    filter(proportion > 0.01) %>%     # More than 1% of the words
    mutate(word = reorder(word,proportion)) %>%  
    ggplot(aes(word,proportion)) + 
    geom_col(aes(fill=word), show.legend = FALSE) + 
    theme_minimal() + 
    coord_flip() + 
    theme(plot.title = element_text(size=10)) +
    labs(title=paste0(mths[j], " ", yrs[j], ": count/total_words"), x="", y="")
  
  # Sentiment scores
  s_score_fed[[j]] <- tidy_MPR %>% 
    mutate(nwords = n()) %>%
    inner_join(Sdict, by="word") %>% 
    add_count(Mood) %>% 
    mutate(n = n/nwords) %>% 
    select(Mood, n) %>% 
    distinct(Mood, .keep_all = TRUE) %>% 
    pivot_wider(names_from = Mood, values_from = n, values_fill = 0) %>% 
    mutate(sentiment=(Positive-Negative), method='FED', Date=date[j])
  
  s_score_b[[j]] <- tidy_MPR %>% 
    mutate(nwords = n()) %>%
    inner_join(get_sentiments('bing'), by="word") %>% 
    add_count(sentiment) %>% 
    mutate(n = n/nwords) %>% 
    select(sentiment, n) %>% 
    distinct(sentiment, .keep_all = TRUE) %>% 
    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
    mutate(sentiment=(positive-negative), method='Bing et al.', Date=date[j])
  
  s_score_a[[j]] <- tidy_MPR %>% 
    inner_join(get_sentiments('afinn'), by="word") %>% 
    summarise(sentiment = sum(value)/n()) %>%
    mutate(method = "AFINN", Date = date[j])

}

s_score_fed <- bind_rows(s_score_fed) 
s_score_a   <- bind_rows(s_score_a) 
s_score_b   <- bind_rows(s_score_b) 

```

## Simple example: 'Inflation' and inflation

- When does an MPC talk about inflation?
- UK data: monthly minutes spanning July 1997 to September 2016 -- 231 roughly ten page documents.
- 407,000 words after data cleaning with nearly 6,700 mentions of 'inflation' or about 1.65% of total word count.
- Policymakers should be discussing inflation below the target as much as above it.
- Turns out for the UK these move together quite strongly _when inflation is a problem_.
- What about Canada?

### Canadian CPI data

Let's get the inflation rate:
```{r cpi, message=FALSE}
download.file(paste("https://fred.stlouisfed.org/series/","/downloaddata/",".csv", 
                    sep="CPALCY01CAQ661N"), destfile = "CACPI.csv")

Inflation <- read_csv("CACPI.csv") %>% 
  mutate(Inflation = 100*(VALUE/lag(VALUE,4) - 1)) %>% 
  filter(year(DATE) > 1994)
```

Let's plot the 'inflation' rate and the inflation rate:
```{r pi}
ggplot(bind_rows(igram)) + 
  geom_area(data=Inflation, aes(x=DATE, y=Inflation), fill="blue", color=NA, alpha=.3) + 
  geom_line(aes(x=Date, y=pi), color="red", size=.9) + 
  theme_minimal() +
  labs(title="Canadian 'inflation' mentions (red) and inflation (blue)", 
       x="", y="% or % of words")
```

## Popular words

What are the most used words in any report? We've already calculated these graphs above, and now we plot them a year at a time using `patchwork`.

Crisis times:
```{r pop}
for (i in 7:10)  {
  ii <- (i-1)*4 + 15
  print((pts[[ii]] | pts[[ii+1]])/(pts[[ii+2]] | pts[[ii+3]]))
}
```

More recent experience:
```{r}
for (i in 17:19)  {
  ii <- (i-1)*4 + 15
  print((pts[[ii]] | pts[[ii+1]])/(pts[[ii+2]] | pts[[ii+3]]))
}
```

## Sentiment 

- Sentiment modeling has become very popular -- for example [SenSR](https://www.dnb.nl/en/binaries/Working%20Paper%20No.%20553_tcm47-356707.pdf).
- Conceptually very simple and we can nicely illustrate the strengths, weaknesses and potential pitfalls.
- Match up the words to a _dictionary_ for example the one in @Loughran: a list of commonly used dictionaries is in @Reagan.
- Then count positive and negative, say, and take a net balance (perhaps as a proportion of total words).
- Some dictionaries give a numeric score (_NRC_); others contain more varied emotions -- _loughran_ specifically for finance includes uncertainty as a sentiment.
- `tidytext` has access to several of these.
- More on dictionaries later.

Jane Austen

```{r Austen, warning=FALSE}
library(janeaustenr)

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number()) %>% 
  ungroup() %>%
  unnest_tokens(word, text)

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing"), by="word") %>%
  count(book, index=linenumber %/% 80, sentiment) %>%  # Count group members
  pivot_wider(names_from=sentiment, values_from=n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  theme_minimal() +  
  facet_wrap(~book, ncol = 2, scales = "free_x") +
  labs(title="Jane Austen's books")
```

### Sentimental mistakes

- Not sensible to use _loughran_ for Jane Austen.
- **Miss** is a very negative word in finance, but not in 1811 polite society.
- We could fix this with a custom dictionary -- Jane Austen uses a lot of words we wouldn't use today.

In the UK we have a number of other contemporary problems:

-    **May** /meI/

    - verb
         1. expressing possibility: "that **may** be true".
         2. used to ask for or to give permission: "you **may** eat the last chocolate biscuit".  

    - noun
         1. the fifth month of the year, in the northern hemisphere usually considered the last month of spring: "we should visit Granny in **May**".
         2. British Prime Minister: "Prime Minister Theresa **May** is in charge of the Brexit negotiations".  

## A financial stability dictionary -- @Correa

- Why a special dictionary?
- All the 'May' caveats exist but also further complications
- @Correa give the example that

> the word 'confined' is classified as having a negative connotation in other dictionaries but almost always conveys a positive sentiment in a financial stability context, as it refers to limiting negative spillovers.
        (@Correa, p. 9)

You only have to look at the final dictionary to find many examples
```{r}
head(Sdict, 10)
```

Question: As dictionary can be applied to any document does it represent FS concerns when applied to MPC deliberations?

```{r senti}
adata <- bind_rows(s_score_b,s_score_a,s_score_fed)

all_s <- ggplot(adata) +
  geom_col(aes(x=Date, y=sentiment, fill=method), show.legend = FALSE) +
  theme_minimal() + 
  facet_wrap(~ method, ncol=1, scales='free_y') + 
  labs(title="Bank of Canada MPR: Sentiment scores", x="", y="")

plot(all_s)
```

## References